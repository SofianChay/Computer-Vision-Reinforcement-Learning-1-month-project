Batch normalization in neural networks :
adjusting and scaling the activations
for the input and hidden layers
improve greatly training speed

Covariance shift
exemple : train on black cats images
	  test on colored cat
	  not do well

batch normalization allows each layer of a network to learn by itself a little bit more independently of other layers.

    We can use higher learning rates because batch normalization makes sure that there’s no activation that’s gone really high or really low. And by that, things that previously couldn’t get to train, it will start to train.
    It reduces overfitting because it has a slight regularization effects. Similar to dropout, it adds some noise to each hidden layer’s activations. Therefore, if we use batch normalization, we will use less dropout, which is a good thing because we are not going to lose a lot of information. However, we should not depend only on batch normalization for regularization; we should better use it together with dropout.

normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation.

atch normalization adds two trainable parameters to each layer, so the normalized output is multiplied by a “standard deviation” parameter (gamma) and add a “mean” parameter (beta).


