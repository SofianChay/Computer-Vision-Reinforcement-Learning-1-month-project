{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "import random\n",
    "import time\n",
    "import numpy as np \n",
    "import skimage.color, skimage.transform\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import os \n",
    "\n",
    "# GROUND TRUTHS SEGMENTATION\n",
    "def encode(labels, labels_figures):\n",
    "    for i in range(len(labels_figures)):\n",
    "        labels[labels == labels_figures[i]] = i\n",
    "    return labels\n",
    "\n",
    "\n",
    "def define_actions(game):\n",
    "    actions = list()\n",
    "    buttons = len(game.get_available_buttons())\n",
    "    for i in range(buttons):\n",
    "        button = [1 if j == i else 0 for j in range(buttons)]\n",
    "        actions.append(button)\n",
    "    return actions\n",
    "\n",
    "\n",
    "def generate_examples(scenario, visualize, batch_size, episodes_train, episodes_val):\n",
    "    \n",
    "    # INIT GAME  \n",
    "    game = DoomGame()\n",
    "    # game.set_vizdoom_path('../../vision-for-action/ViZDoom/bin/python3.7/pip_package/vizdoom')\n",
    "    # game.set_doom_game_path('../../vision-for-action/ViZDoom/bin/python3.7/pip_package/freedoom2.wad')\n",
    "    game.load_config(\"scenarios/\" + scenario)\n",
    "\n",
    "    game.set_window_visible(False)\n",
    "    game.set_render_hud(False)\n",
    "    game.set_mode(Mode.PLAYER)\n",
    "    # Enables labeling of the in game objects.\n",
    "    game.set_labels_buffer_enabled(True)\n",
    "    game.set_screen_resolution(ScreenResolution.RES_84X84) # RES_320X240, RES_640X480, RES_1920X1080\n",
    "    game.set_screen_format(ScreenFormat.GRAY8) # grayscale image  CRCGCB, RGB24, GRAY8, BGR24\n",
    "\n",
    "    game.init()\n",
    "\n",
    "    actions = define_actions(game)\n",
    "    \n",
    "    \n",
    "    # create dict of tensors. validation and train / use pickle file (for the moment)\n",
    "    data = {'train': {'X': list(), 'y': list()}, 'val': {'X': list(), 'y': list()}}\n",
    "    train_X = list()\n",
    "    train_y = list()\n",
    "\n",
    "    # create train set \n",
    "    print('creating train set ...')\n",
    "    labels_figures = [0]\n",
    "    for i in range(episodes_train):\n",
    "        game.new_episode()\n",
    "        print(f'episode {i + 1} running ...')\n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            img = state.screen_buffer\n",
    "            labels = state.labels_buffer\n",
    "            for label in state.labels:\n",
    "                if label.value not in labels_figures:\n",
    "                    labels_figures.append(label.value)\n",
    "            # retrieve data :\n",
    "            train_X.append(torch.from_numpy(img.reshape((1, img.shape[0], img.shape[1])).astype(np.float))) \n",
    "            train_y.append(torch.from_numpy(encode(labels, labels_figures).reshape((1, img.shape[0], img.shape[1]))))\n",
    "            game.make_action(random.choice(actions))\n",
    "\n",
    "\n",
    "        # create val set \n",
    "    print('\\ncreating val set ...')\n",
    "    for i in range(episodes_val):\n",
    "        game.new_episode()\n",
    "        print(f'episode {i + 1} running ...')\n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            img = state.screen_buffer\n",
    "            labels = state.labels_buffer\n",
    "            # retrieve data :\n",
    "            data['val']['X'].append(torch.from_numpy(img.reshape((1, 1, img.shape[0], img.shape[1])).astype(np.float)))\n",
    "            data['val']['y'].append(torch.from_numpy(encode(labels, labels_figures).reshape((1, 1, img.shape[0], img.shape[1]))))\n",
    "            game.make_action(random.choice(actions))\n",
    "       \n",
    "    game.close()\n",
    "\n",
    "    # create batches\n",
    "    print('\\ncreating batches ...')\n",
    "    indices = list(range(len(train_X)))\n",
    "    random.shuffle(indices)\n",
    "    for i in range(len(train_X)//batch_size):\n",
    "        batch_X = list()\n",
    "        batch_y = list()\n",
    "        for j in range(batch_size):\n",
    "            batch_X.append(train_X[indices[i*batch_size + j]])\n",
    "            batch_y.append(train_y[indices[i*batch_size + j]])\n",
    "        batch_X = torch.stack(batch_X)\n",
    "        batch_y = torch.stack(batch_y)\n",
    "        data['train']['X'].append(batch_X)\n",
    "        data['train']['y'].append(batch_y)\n",
    "\n",
    "    # SHOW SOME EXAMPLES\n",
    "    if visualize :\n",
    "        fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(20, 20))\n",
    "        for k in range(5):\n",
    "            tmp = random.choice(range(len(data['val']['X'])))\n",
    "            axes[k][0].imshow(data['val']['X'][tmp][0, 0, :])\n",
    "            axes[k][1].imshow(data['val']['y'][tmp][0, 0, :])\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    return data, labels_figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def contracting_block(self, in_channels, out_channels, kernel_size=3):\n",
    "        block = nn.Sequential(\n",
    "                    nn.Conv2d(kernel_size=kernel_size, in_channels=in_channels, out_channels=out_channels, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.BatchNorm2d(out_channels), \n",
    "                    nn.Conv2d(kernel_size=kernel_size, in_channels=out_channels, out_channels=out_channels, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.BatchNorm2d(out_channels),\n",
    "                    )\n",
    "        return block\n",
    "\n",
    "\n",
    "    def expansive_block(self, in_channels, mid_channels, out_channels, kernel_size=3):\n",
    "        block = nn.Sequential(\n",
    "                    nn.Conv2d(kernel_size=kernel_size, in_channels=in_channels, out_channels=mid_channels),\n",
    "                    nn.ReLU(),\n",
    "                    nn.BatchNorm2d(mid_channels),\n",
    "                    nn.Conv2d(kernel_size=kernel_size, in_channels=mid_channels, out_channels=mid_channels),\n",
    "                    nn.ReLU(mid_channels),\n",
    "                    nn.ConvTranspose2d(in_channels=mid_channels, out_channels=out_channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "                    )\n",
    "        return block\n",
    "\n",
    "    def final_block(self, in_channels, mid_channels, out_channels, kernel_size=3):\n",
    "        block = nn.Sequential(\n",
    "                    nn.Conv2d(kernel_size=kernel_size, in_channels=in_channels, out_channels=mid_channels, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.BatchNorm2d(mid_channels),\n",
    "                    nn.Conv2d(kernel_size=kernel_size, in_channels=mid_channels, out_channels=mid_channels, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.BatchNorm2d(mid_channels),\n",
    "                    nn.Conv2d(kernel_size=kernel_size, in_channels=mid_channels, out_channels=out_channels, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.BatchNorm2d(out_channels)\n",
    "                    # the number of feature maps equal to the number of segments desired.\n",
    "                    )\n",
    "        return block\n",
    "\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        # out_channel represents number of segments desired\n",
    "        super(UNet, self).__init__()\n",
    "        # Encode (contraction)\n",
    "        self.conv_encode1 = self.contracting_block(in_channels=in_channel, out_channels=64)\n",
    "        self.conv_maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv_encode2 = self.contracting_block(64, 128)\n",
    "        self.conv_maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv_encode3 = self.contracting_block(128, 256)\n",
    "        self.conv_maxpool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        # The number of feature maps after each block doubles so that architecture can learn the complex structures \n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "                            nn.Conv2d(kernel_size=3, in_channels=256, out_channels=512),\n",
    "                            nn.ReLU(),\n",
    "                            nn.BatchNorm2d(512),\n",
    "                            nn.Conv2d(kernel_size=3, in_channels=512, out_channels=512),\n",
    "                            nn.ReLU(),\n",
    "                            nn.BatchNorm2d(512),\n",
    "                            nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "                            )\n",
    "\n",
    "\n",
    "        # Decode (expansion)\n",
    "        self.conv_decode3 = self.expansive_block(512, 256, 128) \n",
    "        self.conv_decode2 = self.expansive_block(256, 128, 64)\n",
    "        self.final_layer =  self.final_block(128, 64, out_channel)\n",
    "\n",
    "\n",
    "    def crop_and_concat(self, upsampled, bypass, crop=False):\n",
    "        \"\"\"\n",
    "        The function crop_and_concat appends the output \n",
    "        of contraction layer with the new expansion layer input.\n",
    "        \"\"\"\n",
    "        if crop:\n",
    "            c = (bypass.size()[2] - upsampled.size()[2]) // 2\n",
    "            if (bypass.size()[2] - upsampled.size()[2]) % 2 == 0:\n",
    "                upsampled = F.pad(upsampled, (c, c, c, c))\n",
    "            else:\n",
    "                upsampled = F.pad(upsampled, (c+1, c, c+1, c))\n",
    "        return torch.cat((upsampled, bypass), 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        encode_block1 = self.conv_encode1(x)\n",
    "        encode_pool1 = self.conv_maxpool1(encode_block1)\n",
    "        encode_block2 = self.conv_encode2(encode_pool1)\n",
    "        encode_pool2 = self.conv_maxpool2(encode_block2)\n",
    "        encode_block3 = self.conv_encode3(encode_pool2)\n",
    "        encode_pool3 = self.conv_maxpool3(encode_block3)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck1 = self.bottleneck(encode_pool3)\n",
    "\n",
    "        # Decode\n",
    "        decode_block3 = self.crop_and_concat(bottleneck1, encode_block3, crop=True)\n",
    "        cat_layer2 = self.conv_decode3(decode_block3)\n",
    "        decode_block2 = self.crop_and_concat(cat_layer2, encode_block2, crop=True)\n",
    "        cat_layer1 = self.conv_decode2(decode_block2)\n",
    "        decode_block1 = self.crop_and_concat(cat_layer1, encode_block1, crop=True)\n",
    "        final_layer = self.final_layer(decode_block1)\n",
    "        return  final_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = False\n",
    "scenario = \"basic.cfg\"\n",
    "visualize = True\n",
    "batch_size = 1 \n",
    "episodes_train = 5 \n",
    "episodes_val = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n"
     ]
    },
    {
     "ename": "SharedMemoryException",
     "evalue": "Memory size does not match the the expected size. Possible ViZDoom version mismatch.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSharedMemoryException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3b21eeaa76b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-3b21eeaa76b4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_figures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscenario\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mwidth_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mheight_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-47b1814683c4>\u001b[0m in \u001b[0;36mgenerate_examples\u001b[0;34m(scenario, visualize, batch_size, episodes_train, episodes_val)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_screen_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mScreenFormat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRAY8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# grayscale image  CRCGCB, RGB24, GRAY8, BGR24\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSharedMemoryException\u001b[0m: Memory size does not match the the expected size. Possible ViZDoom version mismatch."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# gpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#########################\n",
    "# training functions\n",
    "def training_step(model, batch_X, batch_y, criterion, optimizer, width_out, height_out, n_classes):\n",
    "\tinputs = batch_X.to(device)\n",
    "\ttruths = batch_y.to(device)\n",
    "\toptimizer.zero_grad()\n",
    "\twith torch.set_grad_enabled(True):\n",
    "\t\toutputs = model(inputs.float())\n",
    "\t\toutputs = outputs.permute(0, 2, 3, 1)\n",
    "\t\t# outputs.shape = (batch_size, n_classes, img_cols, img_rows)\n",
    "\t\tm = outputs.shape[0]      \n",
    "\t\toutputs = outputs.view(m*width_out*height_out, n_classes)\n",
    "\t\t# outputs.shape =(batch_size, img_cols, img_rows, n_classes)\n",
    "\t\ttruths = truths.view(m*width_out*height_out)\n",
    "\t\tloss = criterion(outputs, truths.long())\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\treturn loss\n",
    "\n",
    "def validation_step(model, criterion, val_X, val_y, width_out, height_out, n_classes):\n",
    "\tinputs = val_X.to(device)\n",
    "\ttruths = val_y.to(device)\n",
    "\twith torch.no_grad():\n",
    "\t\toutputs = model(inputs.float())\n",
    "\t\toutputs = outputs.permute(0, 2, 3, 1)\n",
    "\t\t# outputs.shape = (batch_size, n_classes, img_cols, img_rows)\n",
    "\t\tm = outputs.shape[0]\n",
    "\t\toutputs = outputs.view(m*width_out*height_out, n_classes)\n",
    "\t\t# outputs.shape = (batch_size, img_cols, img_rows, n_classes)\n",
    "\t\ttruths = truths.view(m*width_out*height_out)\n",
    "\t\tloss = criterion(outputs, truths.long())\n",
    "\treturn loss\n",
    "\n",
    "def train(model, criterion, optimizer, num_epochs, data, algo, width_out, height_out, n_classes):\n",
    "\tlen_train = len(data['train']['X'])\n",
    "\tlen_val = len(data['val']['X'])\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tprint('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "\t\tprint('-' * 10)\n",
    "\t\ttrain_loss = 0\n",
    "\t\tval_loss = 0\n",
    "\t\tfor phase in ['train', 'val']:\n",
    "\t\t\tif phase == 'train':\n",
    "\t\t\t\tprint(\"learning\")\n",
    "\t\t\t\tmodel.train()\n",
    "\t\t\t\tfor batch_X, batch_y in zip(data['train']['X'], data['train']['y']):\n",
    "\t\t\t\t\ttrain_loss += training_step(model, batch_X, batch_y, criterion, optimizer, width_out, height_out, n_classes)\n",
    "\t\t\t\tprint(f'train loss = {round(float(train_loss / len_train), 4)} ')\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(\"validation\")\n",
    "\t\t\t\tmodel.eval()\n",
    "\t\t\t\tfor val_X, val_y in zip(data['val']['X'], data['val']['y']):\n",
    "\t\t\t\t\tval_loss += validation_step(model, criterion, val_X, val_y, width_out, height_out, n_classes)\n",
    "\t\t\t\tprint(f'validation loss = {round(float(val_loss / len_val), 4)}')\n",
    "\ttorch.save(model.state_dict(), algo + '.pth')\n",
    "\tprint('model saved to ' + algo + '.pth')\n",
    "##########################\n",
    "\n",
    "\n",
    "##########################\n",
    "def plot_examples(model, datax, datay, num_examples, labels_figures):\n",
    "\tmodel.eval()\n",
    "\tfig, ax = plt.subplots(nrows=num_examples, ncols=3, figsize=(18, 4*num_examples))\n",
    "\tm = len(datax)\n",
    "\tfor row_num in range(num_examples):\n",
    "\t\timage_indx = np.random.randint(m)\n",
    "\t\timage_arr = model(datax[image_indx].to(device).float())\n",
    "\t\timage_arr = image_arr.squeeze(0).detach().cpu().numpy()\n",
    "\t\tax[row_num][0].imshow(datax[image_indx][0, 0, :, :])  # show input 1st channel\n",
    "\t\tax[row_num][1].imshow(decode(image_arr.argmax(0), labels_figures))  # show argmax of result of unet\n",
    "\t\tax[row_num][2].imshow(decode(datay[image_indx][0, 0, :, :], labels_figures))  # show ground truth\n",
    "\tplt.show()\n",
    "    \n",
    "def decode(output, labels_figures):\n",
    "    for i in range(len(labels_figures)):\n",
    "        output[output == i] = labels_figures[i]\n",
    "    return output\n",
    "#########################\n",
    "\n",
    "\n",
    "#########################\n",
    "def main():\n",
    "\t# data\n",
    "\tprint(\"loading data\")\n",
    "\tdata, label_figures = generate_examples(scenario, visualize, batch_size, episodes_train, episodes_val)\n",
    "\twidth_in = data['val']['X'][0].shape[2]\n",
    "\theight_in = data['val']['X'][0].shape[3]\n",
    "\tprint(f'input size : {height_in}, {width_in}')\n",
    "\twidth_out = data['val']['y'][0].shape[2]\n",
    "\theight_out = data['val']['y'][0].shape[3]\n",
    "\tprint(f'output size : {height_out}, {width_out}')\n",
    "\tn_classes = len(labels_figures)\n",
    "\tprint(f'number of objects to identify : {n_classes}')\n",
    "\t# parameters\n",
    "\tnum_epochs = 4\n",
    "\t# model\n",
    "\tin_channel =  1 # 1 if gray scale, 3 if RGB\n",
    "\tout_channel = n_classes # number of segments  (depends on the environment) should be len(np.unique(labels))\n",
    "\tmodel = UNet(in_channel, out_channel).to(device)\n",
    "\tprint('model created')\n",
    "\t# criterion\n",
    "\tcriterion = nn.CrossEntropyLoss()\n",
    "\t# optimizer\n",
    "\toptimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.99)\n",
    "\t# algo\n",
    "\talgo = 'segmentation'\n",
    "\t# train\n",
    "\tif train_model:    \n",
    "\t\ttrain(model, criterion, optimizer, num_epochs, data, algo, width_out, height_out, n_classes)\n",
    "\telse:\n",
    "\t\tprint('loading pre-trained model...')\n",
    "\t\tstate_dict = torch.load(algo + '.pth')        \n",
    "\t\tmodel.load_state_dict(state_dict)\n",
    "\tprint(\"showing examples :\")\n",
    "\tplot_examples(model, data['val']['X'], data['val']['y'], 5, labels_figures)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.7'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import vizdoom\n",
    "vizdoom.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
